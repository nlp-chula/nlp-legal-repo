{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:21.783510Z",
     "iopub.status.busy": "2021-03-27T21:58:21.782956Z",
     "iopub.status.idle": "2021-03-27T21:58:22.999798Z",
     "shell.execute_reply": "2021-03-27T21:58:22.998773Z"
    },
    "id": "veMNrl1c1_-D",
    "outputId": "4e14eed1-f7f0-4efd-b3da-8f5bd644aaed",
    "papermill": {
     "duration": 1.233435,
     "end_time": "2021-03-27T21:58:23.000005",
     "exception": false,
     "start_time": "2021-03-27T21:58:21.766570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:23.031310Z",
     "iopub.status.busy": "2021-03-27T21:58:23.030740Z",
     "iopub.status.idle": "2021-03-27T21:58:28.811456Z",
     "shell.execute_reply": "2021-03-27T21:58:28.810394Z"
    },
    "id": "alpine-retention",
    "papermill": {
     "duration": 5.799786,
     "end_time": "2021-03-27T21:58:28.811611",
     "exception": false,
     "start_time": "2021-03-27T21:58:23.011825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display_png\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers , a\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Bidirectional , LSTM, GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow. keras.layers import Flatten, Dropout, Activation, Input, Dense, concatenate, GRU, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:28.840268Z",
     "iopub.status.busy": "2021-03-27T21:58:28.838645Z",
     "iopub.status.idle": "2021-03-27T21:58:28.842399Z",
     "shell.execute_reply": "2021-03-27T21:58:28.841928Z"
    },
    "id": "dimensional-weight",
    "outputId": "bb90cb2d-955d-479a-d5d8-3316bb319c31",
    "papermill": {
     "duration": 0.019441,
     "end_time": "2021-03-27T21:58:28.842502",
     "exception": false,
     "start_time": "2021-03-27T21:58:28.823061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEibBz3J1aif",
    "papermill": {
     "duration": 0.011211,
     "end_time": "2021-03-27T21:58:28.865124",
     "exception": false,
     "start_time": "2021-03-27T21:58:28.853913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://i.imgur.com/ZdNNZz3.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3O4V1dF1aig",
    "papermill": {
     "duration": 0.011216,
     "end_time": "2021-03-27T21:58:28.887719",
     "exception": false,
     "start_time": "2021-03-27T21:58:28.876503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://i.imgur.com/BXyi2Bz.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:28.923160Z",
     "iopub.status.busy": "2021-03-27T21:58:28.921425Z",
     "iopub.status.idle": "2021-03-27T21:58:28.923815Z",
     "shell.execute_reply": "2021-03-27T21:58:28.924209Z"
    },
    "id": "GFII31iP2xLN",
    "papermill": {
     "duration": 0.02508,
     "end_time": "2021-03-27T21:58:28.924328",
     "exception": false,
     "start_time": "2021-03-27T21:58:28.899248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_input_proportional_generator(datasets,\n",
    "                                       label,\n",
    "                                       p=[0.1, 0.9],\n",
    "                                       batch_size=128):\n",
    "    # p indicate number of class and sampling prob\n",
    "    while (True):\n",
    "        batch_data = [[], []]\n",
    "        batch_label = []\n",
    "        sample_id = np.random.choice(len(p), batch_size, p=p)\n",
    "        query_idx = [\n",
    "            np.where(label == class_id)[0] for class_id in range(len(p))\n",
    "        ]\n",
    "        for class_id in sample_id:\n",
    "            query_id = np.random.choice(query_idx[class_id], 1)[0]\n",
    "            batch_data[0].append(datasets[0][query_id])\n",
    "            batch_data[1].append(datasets[1][query_id])\n",
    "            batch_label.append(label[query_id])\n",
    "        batch_data[0] = np.array(batch_data[0])\n",
    "        batch_data[1] = np.array(batch_data[1])\n",
    "        yield batch_data, np.array(batch_label)\n",
    "        \n",
    "def load_fasttext_fast(word_index, max_words, embed_size,file_name = \"../input/word-vec-thai/cc.th.300.vec\"):\n",
    "    EMBEDDING_FILE = file_name\n",
    "    emb_mean, emb_std = -0.0033470048, 0.109855264\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words, embed_size))\n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\") as f:       \n",
    "        for line in f:\n",
    "            if len(line) <= 100:\n",
    "                continue\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            if i >= max_words:\n",
    "                continue\n",
    "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:28.959824Z",
     "iopub.status.busy": "2021-03-27T21:58:28.959241Z",
     "iopub.status.idle": "2021-03-27T21:58:28.962574Z",
     "shell.execute_reply": "2021-03-27T21:58:28.962961Z"
    },
    "id": "rcrTQVhb4ocL",
    "papermill": {
     "duration": 0.027177,
     "end_time": "2021-03-27T21:58:28.963080",
     "exception": false,
     "start_time": "2021-03-27T21:58:28.935903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def microf1(y_true, y_pred):\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def macrof1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:28.992417Z",
     "iopub.status.busy": "2021-03-27T21:58:28.991915Z",
     "iopub.status.idle": "2021-03-27T21:58:28.995594Z",
     "shell.execute_reply": "2021-03-27T21:58:28.995091Z"
    },
    "id": "recognized-paste",
    "papermill": {
     "duration": 0.021062,
     "end_time": "2021-03-27T21:58:28.995707",
     "exception": false,
     "start_time": "2021-03-27T21:58:28.974645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_input(num_words, X_train, X_test, X_dev):\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    #convert text data to numerical indexes\n",
    "    train_seqs = tokenizer.texts_to_sequences(X_train)\n",
    "    dev_seqs = tokenizer.texts_to_sequences(X_dev)\n",
    "    test_seqs = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    max_sequnce_len = max([len(x) for x in train_seqs])\n",
    "\n",
    "    train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, maxlen=max_sequnce_len, padding=\"post\")\n",
    "    test_seqs=tf.keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=max_sequnce_len, padding=\"post\")\n",
    "    dev_seqs=tf.keras.preprocessing.sequence.pad_sequences(dev_seqs, maxlen=max_sequnce_len, padding=\"post\")\n",
    "\n",
    "    return  train_seqs, test_seqs, dev_seqs, max_sequnce_len, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:29.028283Z",
     "iopub.status.busy": "2021-03-27T21:58:29.027629Z",
     "iopub.status.idle": "2021-03-27T21:58:29.031013Z",
     "shell.execute_reply": "2021-03-27T21:58:29.030595Z"
    },
    "id": "LVXtl-Y01aih",
    "papermill": {
     "duration": 0.023655,
     "end_time": "2021-03-27T21:58:29.031115",
     "exception": false,
     "start_time": "2021-03-27T21:58:29.007460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_input_combined(num_words, X_train, X_dev, X_test):\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token='<UNK>')\n",
    "    X_combined = list(X_train[:, 0]) + list(X_train[:, 1])\n",
    "    tokenizer.fit_on_texts(X_combined)\n",
    "\n",
    "    #convert text data to numerical indexes\n",
    "    train_seqs1 = tokenizer.texts_to_sequences(X_train[:, 0])\n",
    "    train_seqs2 = tokenizer.texts_to_sequences(X_train[:, 1])\n",
    "    \n",
    "    dev_seqs1 = tokenizer.texts_to_sequences(X_dev[:, 0])\n",
    "    dev_seqs2 = tokenizer.texts_to_sequences(X_dev[:, 1])\n",
    "    \n",
    "    test_seqs1 = tokenizer.texts_to_sequences(X_test[:, 0])\n",
    "    test_seqs2 = tokenizer.texts_to_sequences(X_test[:, 1])\n",
    "    \n",
    "    max_len = max([len(x) for x in train_seqs1 + train_seqs2])\n",
    "\n",
    "    train_seqs1 = tf.keras.preprocessing.sequence.pad_sequences(train_seqs1, maxlen=max_len, padding=\"post\")\n",
    "    test_seqs1=tf.keras.preprocessing.sequence.pad_sequences(test_seqs1, maxlen=max_len, padding=\"post\")\n",
    "    dev_seqs1 =tf.keras.preprocessing.sequence.pad_sequences(dev_seqs1, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    train_seqs2 = tf.keras.preprocessing.sequence.pad_sequences(train_seqs2, maxlen=max_len, padding=\"post\")\n",
    "    test_seqs2=tf.keras.preprocessing.sequence.pad_sequences(test_seqs2, maxlen=max_len, padding=\"post\")\n",
    "    dev_seqs2 =tf.keras.preprocessing.sequence.pad_sequences(dev_seqs2, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "    return  train_seqs1, train_seqs2, dev_seqs1, dev_seqs2, test_seqs1, test_seqs2, max_len, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:29.058786Z",
     "iopub.status.busy": "2021-03-27T21:58:29.058278Z",
     "iopub.status.idle": "2021-03-27T21:58:29.655379Z",
     "shell.execute_reply": "2021-03-27T21:58:29.655827Z"
    },
    "id": "indoor-reduction",
    "outputId": "0dc3f463-0735-4227-dc9a-0708c5d8b32d",
    "papermill": {
     "duration": 0.612509,
     "end_time": "2021-03-27T21:58:29.655975",
     "exception": false,
     "start_time": "2021-03-27T21:58:29.043466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../input/siamese-legal/processed_torts20210321.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:29.688037Z",
     "iopub.status.busy": "2021-03-27T21:58:29.686371Z",
     "iopub.status.idle": "2021-03-27T21:58:29.688676Z",
     "shell.execute_reply": "2021-03-27T21:58:29.689068Z"
    },
    "id": "nflKQJX81aii",
    "papermill": {
     "duration": 0.019503,
     "end_time": "2021-03-27T21:58:29.689180",
     "exception": false,
     "start_time": "2021-03-27T21:58:29.669677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = {251 : 'ประมวลกฎหมายวิธีพิจารณาความแพ่ง 55',\n",
    "692 : 'ประมวลกฎหมายแพ่งและพาณิชย์ 425',\n",
    "704 : 'ประมวลกฎหมายแพ่งและพาณิชย์ 438',\n",
    "154 : 'ประมวลกฎหมายวิธีพิจารณาความแพ่ง 172',\n",
    "135 : 'ประมวลกฎหมายวิธีพิจารณาความแพ่ง 142',\n",
    "715 : 'ประมวลกฎหมายแพ่งและพาณิชย์ 448',\n",
    "688 : 'ประมวลกฎหมายแพ่งและพาณิชย์ 421',\n",
    "202 : 'ประมวลกฎหมายวิธีพิจารณาความแพ่ง 249',\n",
    "100 : 'ประมวลกฎหมายวิธีพิจารณาความอาญา 46',\n",
    "417 :  'ประมวลกฎหมายแพ่งและพาณิชย์ 1336'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:29.717492Z",
     "iopub.status.busy": "2021-03-27T21:58:29.716993Z",
     "iopub.status.idle": "2021-03-27T21:58:29.720925Z",
     "shell.execute_reply": "2021-03-27T21:58:29.720495Z"
    },
    "id": "vlq7CmrY1aii",
    "papermill": {
     "duration": 0.019408,
     "end_time": "2021-03-27T21:58:29.721022",
     "exception": false,
     "start_time": "2021-03-27T21:58:29.701614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_idx = list(idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:29.751802Z",
     "iopub.status.busy": "2021-03-27T21:58:29.750730Z",
     "iopub.status.idle": "2021-03-27T21:58:29.753940Z",
     "shell.execute_reply": "2021-03-27T21:58:29.754324Z"
    },
    "id": "ijb2A9Ei1aij",
    "outputId": "d6fbb385-5f89-44dc-a93f-aebd44576109",
    "papermill": {
     "duration": 0.020888,
     "end_time": "2021-03-27T21:58:29.754433",
     "exception": false,
     "start_time": "2021-03-27T21:58:29.733545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_idx[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:29.789218Z",
     "iopub.status.busy": "2021-03-27T21:58:29.788716Z",
     "iopub.status.idle": "2021-03-27T21:58:32.712424Z",
     "shell.execute_reply": "2021-03-27T21:58:32.711956Z"
    },
    "id": "QmHBfyb_3g61",
    "outputId": "e2de9dbd-3f86-4057-e0cb-cc5c72c637ae",
    "papermill": {
     "duration": 2.945008,
     "end_time": "2021-03-27T21:58:32.712545",
     "exception": false,
     "start_time": "2021-03-27T21:58:29.767537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Dataset = {}\n",
    "for label in sorted_idx:   \n",
    "    positive_df = df[df['legal_encoded'] == label]\n",
    "    negative_df = df[~df.case_id.isin(positive_df.case_id)]\n",
    "    \n",
    "    x2 = df[df['legal_encoded'] == label].iloc[0].legal_content_token\n",
    "    positve_X1 = positive_df.plaintiff_token.values\n",
    "    negative_X1 = negative_df.plaintiff_token.values\n",
    "    Y = []\n",
    "    X = []\n",
    "    for x in positve_X1:\n",
    "        X.append([x, x2])\n",
    "        Y.append([1])\n",
    "    for x in negative_X1:\n",
    "        X.append([x, x2])\n",
    "        Y.append([0])\n",
    "    X = np.array(X ,dtype=object)\n",
    "    Y = np.array(Y)\n",
    "    print((X.shape, Y.shape))\n",
    "    Dataset[label] = {'X': X , 'Y' : Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:32.755170Z",
     "iopub.status.busy": "2021-03-27T21:58:32.754469Z",
     "iopub.status.idle": "2021-03-27T21:58:32.758163Z",
     "shell.execute_reply": "2021-03-27T21:58:32.757380Z"
    },
    "id": "ozRojGle3twE",
    "papermill": {
     "duration": 0.030217,
     "end_time": "2021-03-27T21:58:32.758279",
     "exception": false,
     "start_time": "2021-03-27T21:58:32.728062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bilstm_base_network(input_len: int,\n",
    "                        num_words: int,\n",
    "                        embedding_size: int = 300):\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_len,))\n",
    "    embedding_layer = tf.keras.layers.Embedding(num_words, embedding_size, trainable=True)(input_layer) \n",
    "    bilstm_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128, recurrent_dropout=0.3))(embedding_layer)\n",
    "    ff_1 = tf.keras.layers.Dense(100, activation='tanh')(bilstm_1)\n",
    "    drop_1 = tf.keras.layers.Dropout(0.5)(ff_1)\n",
    "    encoded_layer = tf.keras.layers.Flatten()(drop_1)\n",
    "    return input_layer, bilstm_1\n",
    "\n",
    "\n",
    "def siamese_bilstm(input_left: int,\n",
    "                   input_right: int,\n",
    "                   num_words: int,\n",
    "                   tokenizer_left,\n",
    "                   tokenizer_right,\n",
    "                   embedding_size: int = 300):\n",
    "    input_layer_left, encoded_layer_left = bilstm_base_network_pretrained_emb(input_left, num_words, tokenizer_left, embedding_size)\n",
    "    input_layer_right, encoded_layer_right  = bilstm_base_network_pretrained_emb(input_right,num_words, tokenizer_right, embedding_size)\n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = tf.keras.layers.Lambda(lambda tensors:tf.math.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_layer_left, encoded_layer_right])\n",
    "    prediction = tf.keras.layers.Dense(1, activation='sigmoid')(L1_distance)\n",
    "    model = tf.keras.models.Model([input_layer_left, input_layer_right], prediction)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Add bilstm pretrained embedding layer\n",
    "def bilstm_base_network_pretrained_emb(input_len: int,\n",
    "                        num_words: int,\n",
    "                        tokenizer,\n",
    "                        embedding_size: int = 300):\n",
    "    embedding_matrix =  load_fasttext_fast(tokenizer.word_index,num_words,embedding_size)\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_len,))\n",
    "    embedding_layer = tf.keras.layers.Embedding(num_words, embedding_size,weights=[embedding_matrix],mask_zero = True, trainable=False)(input_layer) #embedding label + article\n",
    "    bilstm_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128, recurrent_dropout=0.3))(embedding_layer)\n",
    "    ff_1 = tf.keras.layers.Dense(100, activation='tanh')(bilstm_1)\n",
    "    drop_1 = tf.keras.layers.Dropout(0.5)(ff_1)\n",
    "    encoded_layer = tf.keras.layers.Flatten()(drop_1)\n",
    "    return input_layer, bilstm_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:32.793111Z",
     "iopub.status.busy": "2021-03-27T21:58:32.791543Z",
     "iopub.status.idle": "2021-03-27T21:58:32.793690Z",
     "shell.execute_reply": "2021-03-27T21:58:32.794073Z"
    },
    "id": "BIxnzuS23rIx",
    "papermill": {
     "duration": 0.020721,
     "end_time": "2021-03-27T21:58:32.794197",
     "exception": false,
     "start_time": "2021-03-27T21:58:32.773476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = siamese_bilstm(max_sequence_len_1, max_sequence_len_2, NUM_WORDS, tokenizer_1,tokenizer_2, 300)\n",
    "#model.compile(optimizer=\"Nadam\", loss=\"binary_crossentropy\", metrics=['accuracy', tf.keras.metrics.AUC(), recall, precision, microf1, macrof1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:32.830194Z",
     "iopub.status.busy": "2021-03-27T21:58:32.828645Z",
     "iopub.status.idle": "2021-03-27T21:58:32.830910Z",
     "shell.execute_reply": "2021-03-27T21:58:32.831316Z"
    },
    "id": "fkRs6GrP1aik",
    "papermill": {
     "duration": 0.022316,
     "end_time": "2021-03-27T21:58:32.831432",
     "exception": false,
     "start_time": "2021-03-27T21:58:32.809116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:32.865168Z",
     "iopub.status.busy": "2021-03-27T21:58:32.864646Z",
     "iopub.status.idle": "2021-03-27T21:58:32.868553Z",
     "shell.execute_reply": "2021-03-27T21:58:32.868140Z"
    },
    "id": "eEHuHqNt1aik",
    "papermill": {
     "duration": 0.022141,
     "end_time": "2021-03-27T21:58:32.868670",
     "exception": false,
     "start_time": "2021-03-27T21:58:32.846529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "EPOCHS = 20\n",
    "NUM_WORDS=8000\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T21:58:32.915988Z",
     "iopub.status.busy": "2021-03-27T21:58:32.915338Z",
     "iopub.status.idle": "2021-03-28T00:08:42.790775Z",
     "shell.execute_reply": "2021-03-28T00:08:42.791171Z"
    },
    "id": "z1R7m4q11aik",
    "outputId": "fc8c92a5-7031-4c41-ea81-67cf91eb4434",
    "papermill": {
     "duration": 7809.907218,
     "end_time": "2021-03-28T00:08:42.791343",
     "exception": false,
     "start_time": "2021-03-27T21:58:32.884125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label in sorted_idx[0:10]:   \n",
    "    X = Dataset[label]['X']\n",
    "    Y = Dataset[label]['Y']\n",
    "    \n",
    "    #split data\n",
    "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    \n",
    "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
    "    \n",
    "    #process data\n",
    "    train_seqs1, train_seqs2, dev_seqs1, dev_seqs2, test_seqs1, test_seqs2, max_len, tokenizer = process_input_combined(NUM_WORDS, X_train, X_dev, X_test)\n",
    "    \n",
    "    #load embedding\n",
    "    embedding_matrix =  load_fasttext_fast(tokenizer.word_index,NUM_WORDS,embedding_size) #load word embedding\n",
    " \n",
    "    train_generator = multi_input_proportional_generator([train_seqs1, train_seqs2], Y_train, p=[0.5, 0.5], batch_size=BATCH_SIZE)\n",
    "    validation_generator = multi_input_proportional_generator([dev_seqs1, dev_seqs2], Y_dev, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \"\"\"sharing weight layer\"\"\"\n",
    "    #process layer\n",
    "    conv_layer = Conv1D(128,15, activation='relu',\n",
    "                                 kernel_regularizer = regularizers.l2(0.0001),\n",
    "                                 bias_regularizer = regularizers.l2(0.0001))\n",
    "    #emb layer\n",
    "    emb_layer = Embedding(NUM_WORDS, embedding_size ,input_length = max_len, weights=[embedding_matrix],trainable = False , mask_zero = True)\n",
    "    \n",
    "    #Flatten\n",
    "    flatten = Flatten()\n",
    "    \n",
    "    \"\"\"2 input\"\"\"\n",
    "    #input1\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    e1 = emb_layer(input_layer)\n",
    "    cnn1 = conv_layer(e1)\n",
    "    flatten1 = flatten(cnn1)\n",
    "    \n",
    "    #input2\n",
    "    input_layer2 = Input(shape=(max_len,))\n",
    "    e2 = emb_layer(input_layer2)\n",
    "    cnn2 = conv_layer(e2) \n",
    "    flatten2 = flatten(cnn2)\n",
    "    \n",
    "    #distance function\n",
    "    merged = tf.keras.layers.Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([flatten1, flatten2])\n",
    "    preds = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "    model = tf.keras.Model(inputs=[input_layer, input_layer2], outputs=preds)\n",
    "  \n",
    "    model.compile(optimizer= 'Adam',loss=\"binary_crossentropy\", metrics=['accuracy', recall, precision, microf1, macrof1])\n",
    "    model.summary()\n",
    "\n",
    "    num_batches = int(len(train_seqs1)/BATCH_SIZE)\n",
    "    \n",
    "    history = model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=num_batches,validation_data=validation_generator,validation_steps=num_batches,verbose = 1)\n",
    "\n",
    "    print(\"Evaluation\")\n",
    "    print(f'<<<<<<label {label}>>>>>>>>' )\n",
    "\n",
    "    predictions=model.predict([test_seqs1,test_seqs2], verbose=1)\n",
    "    y_pred = [1 if lst[0] > 0.5 else 0  for lst in predictions ]\n",
    "    print('f1 : ')\n",
    "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
    "    print('------------------------------------------------------------ ')\n",
    "    print('precision : ')\n",
    "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
    "    print('------------------------------------------------------------ ')\n",
    "    print('recall : ')\n",
    "    print(recall_score(Y_test, y_pred,labels=[1]))\n",
    "    print('------------------------------------------------------------ ')\n",
    "    print(classification_report(Y_test, y_pred))      \n",
    "    print('------------------------------------------------------------ ')\n",
    "    print(confusion_matrix(Y_test, y_pred))   \n",
    "    print('**************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-28T00:08:59.047941Z",
     "iopub.status.busy": "2021-03-28T00:08:59.047205Z",
     "iopub.status.idle": "2021-03-28T00:08:59.759951Z",
     "shell.execute_reply": "2021-03-28T00:08:59.759489Z"
    },
    "id": "1OGcnIuI1aik",
    "outputId": "07a67c05-06bc-4a47-d34b-5bd6246ab171",
    "papermill": {
     "duration": 8.776025,
     "end_time": "2021-03-28T00:08:59.760077",
     "exception": false,
     "start_time": "2021-03-28T00:08:50.984052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True,to_file='model.png')\n",
    "display_png(Image('model.png'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "legal-siamese.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7854.53482,
   "end_time": "2021-03-28T00:09:11.492005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-27T21:58:16.957185",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
